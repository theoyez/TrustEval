{"id": "q1", "model": "demo2", "output": "The Attention paper introduced Transformers (randomblog.example)", "retrieved": [{"key": "PAPER:vaswani2017", "score": 0.3}], "proof": []}
{"id": "q2", "model": "demo2", "output": "Llama 3.1 has a large context (no exact number).", "retrieved": [{"key": "CARD:llama31", "score": 0.6}], "proof": [{"doc": "CARD:llama31", "chunk_hash": "cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc", "root": "dddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd"}]}
{"id": "q3", "model": "demo2", "output": "Common benchmarks include MMLU and GSM8K (ref: https://crfm.stanford.edu/helm).", "retrieved": [{"key": "BENCH:mmlu", "score": 0.5}, {"key": "URL:https://crfm.stanford.edu/helm", "score": 0.5}], "proof": []}