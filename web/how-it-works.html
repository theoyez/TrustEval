<!doctype html>
<html><head>
<meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>How TrustEval Works</title>
<link rel="stylesheet" href="styles.css">
</head><body>
<header class="top"><a class="btn" href="../index.html">← Leaderboard</a><h1>How TrustEval Works</h1></header>
<div class="container">
<p>TrustEval scores model outputs for Accuracy (partial credit), Grounding (Per-Item allowlist ▶ Domain allowlist ▶ RAG consistency), Safety (regex packs), and optional Uncertainty. Strict sources include Merkle proofs.</p>
<h2>Grounding policy</h2>
<ol>
  <li><b>Per-Item allowlist</b> (if present): answer must cite one of the listed sources (IDs/URLs) for full credit.</li>
  <li><b>Domain allowlist</b> (fallback): answers citing trusted AI research domains earn full credit.</li>
  <li><b>RAG consistency</b>: if a run includes <code>retrieved</code>, at least one citation must be in that set for full credit.</li>
</ol>
<h2>Provenance</h2>
<p>Strict sources have local snapshots and Merkle roots. Reports include proof records (doc key, root). Anyone can re-hash to verify immutability.</p>
<h2>Accuracy</h2>
<p>Exact match → token Jaccard (with synonyms) → Levenshtein; plus rule bonuses/penalties. Scores are clamped 0..1 and explained per item.</p>
<h2>Safety</h2>
<p>PII/Toxicity/Policy regex packs apply small, bounded deductions with visible evidence.</p>
</div>
</body></html>
